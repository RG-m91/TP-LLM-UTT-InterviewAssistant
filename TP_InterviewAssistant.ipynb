{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "933703df-7418-444f-9206-131ff1372615",
   "metadata": {},
   "source": [
    "# TP1 : Introduction aux LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be280aa4-b908-4406-8565-e4f5041cb36a",
   "metadata": {},
   "source": [
    "## 1. Installation des dépendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18881969-3d88-43cb-aa0b-1acbb486fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio openai \n",
    "!pip install transformers datasets torch datasets\n",
    "!pip install scikit-learn pandas\n",
    "!pip install fpdf fitz frontend pillow pdfplumber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c400b3-5164-4522-b069-309673f99563",
   "metadata": {},
   "source": [
    "## Instanciation d'un modèle LLM depuis Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1007b894-fde2-46b7-a9cc-debaae31bedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crée un client qui se connecte à l'API Groq via  API key\n",
    "import os\n",
    "import openai\n",
    "\n",
    "# prérequis: créer un compte  et générer un token https://console.groq.com/keys\n",
    "\n",
    "if not os.environ.get(\"GROQ_API_KEY\"):\n",
    "  os.environ[\"GROQ_API_KEY\"] = getpass.getpass('Enter your groq api key')\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    base_url=\"https://api.groq.com/openai/v1\",\n",
    "    api_key=os.environ[\"GROQ_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b179b7f-076c-4450-9d1f-506206e052bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_choosed=\"meta-llama/llama-4-scout-17b-16e-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd368ee-4749-4ffd-b8f2-8cd8d0433900",
   "metadata": {},
   "source": [
    "## A. Simulation d'Entretien "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3427909d-3489-4eab-aa60-936dfb72b78a",
   "metadata": {},
   "source": [
    "•\tGénération de Questions : Utilisation d’un LLM léger pour générer dynamiquement des questions d’entretien en fonction du poste ou du profil souhaité. \n",
    "•\tInteraction Conversationnelle : L’assistant peut jouer le rôle d’un recruteur en posant des questions et en enregistrant les réponses du candidat.\n",
    "\n",
    "==> Génération de questions dynamiques à l’aide d’un LLM léger local (ou via transformers en fallback)\n",
    "\n",
    "==> Interaction sous forme de Q/R avec transcription manuelle simulée (ou audio à ajouter ensuite)\n",
    "\n",
    "==> Stockage des questions et réponses dans un log structuré (en vue d’une future analyse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fb4f4c-9248-4b2c-98de-b3132cb86938",
   "metadata": {},
   "source": [
    "Prérequis :\n",
    "    -transformers (pip install transformers)\n",
    "    -(Optionnel) un LLM local comme Mistral ou TinyLLaMA via Ollama ou llama-cpp-python\n",
    "    -whisper pour l’audio + transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a39b37d-2ec7-4373-96ff-218158863145",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e13315-accd-47c8-9d97-8c3705ab92bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44921c18-3616-4285-b507-66036248f91d",
   "metadata": {},
   "source": [
    "### Chat with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67e8440-c847-4518-84bf-1fe68c65854b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction générique pour interagir avec le modèle en streaming\n",
    "def chat_with_model(prompt, display=True):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    stream = client.chat.completions.create(\n",
    "        model=model_choosed,  \n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        content = chunk.choices[0].delta.content\n",
    "        if content:\n",
    "            response += content\n",
    "            if display:\n",
    "                print(content, end=\"\")\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43071ba3-4d85-4c50-af01-22997a672d3a",
   "metadata": {},
   "source": [
    "### Fonction test si API / requête modele fonctionne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01921ffe-aea1-44b2-8326-acf7f9dbc2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt= \"Peux-tu me raconter une blague ? \"\n",
    "reponse = chat_with_model(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85a3066-ab9d-418b-bfe4-b6f7a7037c00",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Simulation interactive de l’entretien"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da194f8f-cf2a-4bcc-ae5e-b0b14e5297a7",
   "metadata": {},
   "source": [
    "### Générateur de questions par rapport à un poste avec Interface gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8899268-f9c4-49b9-9347-6cdb27b098d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Variables de session\n",
    "session_log = {\n",
    "    \"profil\": \"\",\n",
    "    \"questions_reponses\": [],\n",
    "    \"previous_qs\": [],\n",
    "}\n",
    "\n",
    "# Génération de question (même logique que plus tôt)\n",
    "def generate_question_gradio(profil, model):\n",
    "    global model_choosed\n",
    "    model_choosed = model\n",
    "    prompt = f\"\"\"Tu es un recruteur expérimenté. Pose une nouvelle question d'entretien à un candidat pour un poste de {profil}.\n",
    "Évite de répéter les questions précédentes : {', '.join(session_log['previous_qs']) if session_log['previous_qs'] else 'aucune'}.\n",
    "Donne uniquement la question, sans introduction, sans commentaire.\"\"\"\n",
    "    \n",
    "    question = chat_with_model(prompt, display=False).strip()\n",
    "    session_log[\"previous_qs\"].append(question)\n",
    "    return question\n",
    "\n",
    "# Fonction principale d’interaction (question ↔ réponse)\n",
    "def entretien_interactif(profil, model, reponse_utilisateur):\n",
    "    if not session_log[\"profil\"]:\n",
    "        session_log[\"profil\"] = profil\n",
    "\n",
    "    if reponse_utilisateur and session_log[\"questions_reponses\"]:\n",
    "        # Sauvegarde réponse précédente\n",
    "        session_log[\"questions_reponses\"][-1][\"reponse\"] = reponse_utilisateur\n",
    "\n",
    "    # Génère une nouvelle question\n",
    "    question = generate_question_gradio(profil, model)\n",
    "    session_log[\"questions_reponses\"].append({\n",
    "        \"question\": question,\n",
    "        \"reponse\": \"\"\n",
    "    })\n",
    "    return question\n",
    "\n",
    "# Fonction pour exporter les logs\n",
    "def exporter_log():\n",
    "    file_name = f\"entretien_{session_log['profil'].replace(' ', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(session_log, f, indent=2, ensure_ascii=False)\n",
    "    return file_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69812ed1-db6f-420a-bd21-95bde1fbd6c0",
   "metadata": {},
   "source": [
    "### Fonction analyse automatique des réponses (LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a0e865-7fd7-4b62-8f93-4ef4acdf3057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyser_entretien():\n",
    "    if not session_log[\"questions_reponses\"]:\n",
    "        return \"Aucune réponse à analyser.\"\n",
    "\n",
    "    # Génère le contenu brut de l'entretien pour analyse\n",
    "    entretien_text = \"\\n\".join([\n",
    "        f\"Q: {pair['question']}\\nR: {pair['reponse']}\" \n",
    "        for pair in session_log[\"questions_reponses\"]\n",
    "    ])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Tu es un expert RH. Analyse les réponses suivantes données lors d’un entretien pour un poste de {session_log['profil']} :\n",
    "\n",
    "{entretien_text}\n",
    "\n",
    "Donne un retour constructif :\n",
    "- Points forts\n",
    "- Points à améliorer\n",
    "- Conseils pour le prochain entretien\n",
    "Réponds de façon bienveillante et professionnelle.\n",
    "\"\"\"\n",
    "    feedback = chat_with_model(prompt, display=False)\n",
    "    return feedback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee5547a-f76c-4e83-9197-a23310b91438",
   "metadata": {},
   "source": [
    "### Fonction Évaluation des soft skills détectées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70524540-15a1-4581-88be-373224984ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_soft_skills():\n",
    "    entretien_text = \"\\n\".join([\n",
    "        f\"Q: {pair['question']}\\nR: {pair['reponse']}\" \n",
    "        for pair in session_log[\"questions_reponses\"]\n",
    "    ])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Lis cet échange d’entretien pour un poste de {session_log['profil']} :\n",
    "\n",
    "{entretien_text}\n",
    "\n",
    "Identifie les soft skills perçues dans les réponses du candidat (exemples : communication, leadership, adaptabilité, résolution de problèmes...).\n",
    "Présente les résultats sous forme de liste avec un niveau estimé pour chaque compétence (faible, moyen, fort) et un commentaire.\n",
    "\"\"\"\n",
    "    soft_skills_feedback = chat_with_model(prompt, display=False)\n",
    "    return soft_skills_feedback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca475bc-d1a2-4c43-9a34-fc4e701a440a",
   "metadata": {},
   "source": [
    "### Génération de note globale de performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec562b3-bde3-441d-9a64-8d23287765ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generer_note_globale():\n",
    "    entretien_text = \"\\n\".join([\n",
    "        f\"Q: {pair['question']}\\nR: {pair['reponse']}\" \n",
    "        for pair in session_log[\"questions_reponses\"]\n",
    "    ])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Voici un entretien pour un poste de {session_log['profil']} :\n",
    "\n",
    "{entretien_text}\n",
    "\n",
    "Attribue une note globale de performance (sur 10) au candidat, suivie d'un résumé de l'évaluation. Sois professionnel et bienveillant.\n",
    "Exemple de sortie : \"Note : 7/10 - Bon niveau technique, mais manque de précision sur certains points...\"\n",
    "\"\"\"\n",
    "    result = chat_with_model(prompt, display=False)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d753a8-0a21-4adf-9bda-18d3f996d877",
   "metadata": {},
   "source": [
    "### Génération + Téléchargement d’un PDF récapitulatif "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cde2f8-d3f7-4e22-8ef7-9a4182c43dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpdf import FPDF\n",
    "import tempfile\n",
    "\n",
    "def generer_pdf_entretien():\n",
    "    pdf = FPDF()\n",
    "    pdf.add_page()\n",
    "    pdf.set_auto_page_break(auto=True, margin=15)\n",
    "\n",
    "    pdf.set_font(\"Arial\", 'B', 16)\n",
    "    pdf.cell(0, 10, f\"Compte-rendu d'entretien - {session_log['profil']}\", ln=True)\n",
    "\n",
    "    pdf.set_font(\"Arial\", size=12)\n",
    "    pdf.ln(5)\n",
    "\n",
    "    for i, pair in enumerate(session_log[\"questions_reponses\"], 1):\n",
    "        pdf.multi_cell(0, 10, f\"Q{i} : {pair['question']}\")\n",
    "        pdf.multi_cell(0, 10, f\"R{i} : {pair['reponse']}\")\n",
    "        pdf.ln(2)\n",
    "\n",
    "    # Feedback RH\n",
    "    feedback = analyser_entretien()\n",
    "    pdf.set_font(\"Arial\", 'B', 14)\n",
    "    pdf.cell(0, 10, \"🧠 Feedback RH\", ln=True)\n",
    "    pdf.set_font(\"Arial\", size=12)\n",
    "    pdf.multi_cell(0, 10, feedback)\n",
    "\n",
    "    # Soft Skills\n",
    "    softskills = eval_soft_skills()\n",
    "    pdf.set_font(\"Arial\", 'B', 14)\n",
    "    pdf.cell(0, 10, \"📊 Soft Skills détectées\", ln=True)\n",
    "    pdf.set_font(\"Arial\", size=12)\n",
    "    pdf.multi_cell(0, 10, softskills)\n",
    "\n",
    "    # Note globale\n",
    "    note = generer_note_globale()\n",
    "    pdf.set_font(\"Arial\", 'B', 14)\n",
    "    pdf.cell(0, 10, \"🎯 Note globale de performance\", ln=True)\n",
    "    pdf.set_font(\"Arial\", size=12)\n",
    "    pdf.multi_cell(0, 10, note)\n",
    "\n",
    "    # Sauvegarde temporaire\n",
    "    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\")\n",
    "    pdf.output(temp_file.name)\n",
    "    return temp_file.name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb17fcf-caf4-48be-a22d-9ab21c9b03fe",
   "metadata": {},
   "source": [
    "### Fonctionnalité : Upload de photo du candidat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2649ac17-09f9-4c12-9416-7f88f46dae26",
   "metadata": {},
   "source": [
    "Exemples de consignes personnalisables :\n",
    "“Cette personne inspire-t-elle confiance pour un entretien ?”\n",
    "\n",
    "“Cette tenue est-elle adaptée à un poste en entreprise ?”\n",
    "\n",
    "“Fais une analyse non-verbale comme un coach en image.”"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fae82c84-104f-44e4-b5f5-a3778730b2d7",
   "metadata": {},
   "source": [
    "1 token ≈ 4 caractères → 100 Ko JPEG → ~100k tokens 😬\n",
    "Or Groq (comme OpenAI) limite à 32 768 tokens max pour inputs + outputs\n",
    "==> forcer une image < 30 Ko encodée en base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0729006a-0c3e-4980-aead-fa0f935eeb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def analyser_photo_candidat(image_path, commentaire=\"Analyse cette photo pour un entretien : posture, tenue, expression.\"):\n",
    "    if not image_path:\n",
    "        return \"Aucune image chargée.\"\n",
    "\n",
    "    # Encode l'image en base64\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        image_bytes = f.read()\n",
    "        image_b64 = base64.b64encode(image_bytes).decode('utf-8')\n",
    "    \n",
    "    image_data_url = f\"data:image/jpeg;base64,{image_b64}\"  # ou image/png selon le type\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": commentaire\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": image_data_url\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Appel au modèle Vision (ex: llava-1.5-7b ou autre)\n",
    "    stream = client.chat.completions.create(\n",
    "        model=model_choosed,  # modèle vision compatible Groq\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    result = \"\"\n",
    "    for chunk in stream:\n",
    "        content = chunk.choices[0].delta.content\n",
    "        if content:\n",
    "            result += content\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1399cd05-6a9e-43ec-8167-277ca2bcea91",
   "metadata": {},
   "source": [
    "### [KO] Fonctionnalité : Upload de photo du candidat avec compression si > 30ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9738609-709f-4562-bd5f-a640cad01841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: KO compression\n",
    "def analyser_photo_candidat_avec_preview(image_path, commentaire=\"Analyse cette photo pour un entretien : posture, tenue, expression.\"):\n",
    "    if not image_path:\n",
    "        return \"Aucune image chargée.\", None\n",
    "\n",
    "    # Étape 1 : Compression + redimensionnement\n",
    "    with Image.open(image_path) as img:\n",
    "        img = img.convert(\"RGB\")\n",
    "        img = img.resize((224, 224))\n",
    "        buffered = io.BytesIO()\n",
    "        img.save(buffered, format=\"JPEG\", quality=30, optimize=True)\n",
    "        image_bytes = buffered.getvalue()\n",
    "\n",
    "        # Pour Gradio, on retourne un objet PIL directement ici\n",
    "        img_preview = Image.open(io.BytesIO(image_bytes))\n",
    "\n",
    "    # Vérif taille\n",
    "    compressed_size_kb = len(image_bytes) / 1024\n",
    "    if compressed_size_kb > 40:\n",
    "        return f\"❌ Image compressée trop lourde ({compressed_size_kb:.1f} Ko).\", img_preview\n",
    "\n",
    "    # Encodage base64 pour le LLM\n",
    "    image_b64 = base64.b64encode(image_bytes).decode('utf-8')\n",
    "    image_data_url = f\"data:image/jpeg;base64,{image_b64}\"\n",
    "\n",
    "    # Construction du message pour le LLM\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": commentaire},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": image_data_url}}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Appel API\n",
    "    stream = client.chat.completions.create(\n",
    "        model=model_choosed,\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    result = \"\"\n",
    "    for chunk in stream:\n",
    "        content = chunk.choices[0].delta.content\n",
    "        if content:\n",
    "            result += content\n",
    "\n",
    "    return result, img_preview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b753085f-635e-4255-9383-f727c9400eb7",
   "metadata": {},
   "source": [
    "### Charger une fiche de poste (texte)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "018026f4-d108-4211-ac70-12e45be22954",
   "metadata": {},
   "source": [
    "Générer des questions spécifiques au poste\n",
    "Adapter l’analyse des réponses à ce profil\n",
    "Enrichir la synthèse RH et le rapport PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5c4217-1ecf-47eb-879d-7bed4d228eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traiter_fiche_de_poste(texte_fiche, objectif=\"Génère 5 questions d'entretien adaptées à ce poste\"):\n",
    "    if not texte_fiche.strip():\n",
    "        return \"🛑 Merci de coller le contenu de la fiche de poste.\"\n",
    "\n",
    "    # 🔒 Limiter la taille du texte pour éviter les erreurs du modèle\n",
    "    max_chars = 4000  # Ajustable selon le modèle\n",
    "    if len(texte_fiche) > max_chars:\n",
    "        return f\"⚠️ La fiche est trop longue ({len(texte_fiche)} caractères). Limite : {max_chars}.\"\n",
    "\n",
    "    try:\n",
    "        prompt = f\"{objectif}\\n\\nVoici la fiche de poste :\\n{texte_fiche}\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "\n",
    "        stream = client.chat.completions.create(\n",
    "            model=model_choosed,\n",
    "            messages=messages,\n",
    "            stream=True\n",
    "        )\n",
    "\n",
    "        result = \"\"\n",
    "        for chunk in stream:\n",
    "            content = chunk.choices[0].delta.content\n",
    "            if content:\n",
    "                result += content\n",
    "\n",
    "        return result if result.strip() else \"⚠️ Réponse vide du modèle.\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"❌ Erreur : {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80afbd3f-15f0-4ba3-b2bb-6c40cc332846",
   "metadata": {},
   "source": [
    "### [KO] Charger une fiche de poste (pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7f03a7-29d1-4780-83b1-bf0eed4fb288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF pour lire les PDF\n",
    "\n",
    "def lire_fiche_de_poste(fichier):\n",
    "    if not fichier:\n",
    "        return \"Aucun fichier fourni.\"\n",
    "\n",
    "    content = \"\"\n",
    "    if fichier.name.endswith(\".pdf\"):\n",
    "        doc = fitz.open(fichier.name)\n",
    "        content = \"\\n\".join([page.get_text() for page in doc])\n",
    "    elif fichier.name.endswith(\".txt\"):\n",
    "        with open(fichier.name, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "    else:\n",
    "        return \"Format non supporté. Utilisez un PDF ou un .txt.\"\n",
    "\n",
    "    # On résume le contenu avec le LLM\n",
    "    prompt = f\"\"\"\n",
    "Voici une fiche de poste. Résume-la en identifiant :\n",
    "- Le poste\n",
    "- Les compétences clés attendues\n",
    "- Les missions principales\n",
    "- Le profil idéal\n",
    "\n",
    "Fiche :\n",
    "{content}\n",
    "\"\"\"\n",
    "    resume = chat_with_model(prompt, display=False)\n",
    "\n",
    "    # On stocke dans la session pour l’utiliser dans la génération des questions\n",
    "    session_log[\"profil\"] = resume\n",
    "\n",
    "    return resume\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a4cc28-b909-46b7-a441-d2d3bcadc17b",
   "metadata": {},
   "source": [
    "### [TODO] intégration fiche de poste dans la génération des questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca91fad-444a-49d0-ab8f-21457aae3645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intégration dans la génération des questions\n",
    "prompt = f\"\"\"\n",
    "Tu joues le rôle d'un recruteur. Génère une question d'entretien adaptée à ce profil :\n",
    "\n",
    "{session_log['profil']}\n",
    "\n",
    "Pose une question à la fois. N’indique que la question sans commentaire ni réponse.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e686048-37ee-4e25-9f1a-331803ad10b6",
   "metadata": {},
   "source": [
    "### Interface utilisateur GRADIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581abb93-34a0-4cbb-b2ae-2bba05441227",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks(title=\"Assistant d'entretien IA\") as demo:\n",
    "    gr.Markdown(\"## 🤖 Assistant IA d'entraînement à l'entretien d'embauche\")\n",
    "\n",
    "    with gr.Row():\n",
    "        poste_input = gr.Textbox(label=\"Profil / Poste visé\", placeholder=\"ex: ingénieur IA, développeur web...\")\n",
    "        model_input = gr.Dropdown([model_choosed], \n",
    "                                  label=\"Modèle utilisé\", value=model_choosed)\n",
    "\n",
    "    # analyse photo\n",
    "    gr.Markdown(\"## 📸 Analyse Visuelle du Candidat\")\n",
    "    photo_input = gr.Image(label=\"📷 Charge une photo\", type=\"filepath\")\n",
    "    feedback_photo = gr.Textbox(label=\"🧠 Analyse de la photo\", lines=5)\n",
    "    bouton_photo = gr.Button(\"Analyser la photo\")\n",
    "\n",
    "    bouton_photo.click(fn=analyser_photo_candidat, inputs=[photo_input], outputs=[feedback_photo])\n",
    "\n",
    "    # Upload fiche de post sous txt\n",
    "    gr.Markdown(\"## 📋 Analyse de fiche de poste (coller le texte)\")\n",
    "    \n",
    "    fiche_input = gr.Textbox(lines=10, placeholder=\"Collez ici le contenu de la fiche de poste\", label=\"📝 Fiche de poste\")\n",
    "    objectif_input = gr.Textbox(value=\"Génère 5 questions d'entretien adaptées à ce poste\", label=\"🎯 Objectif de génération\")\n",
    "    \n",
    "    bouton = gr.Button(\"🔍 Analyser\")\n",
    "    output = gr.Textbox(label=\"📊 Résultat\")\n",
    "\n",
    "    bouton.click(\n",
    "        fn=traiter_fiche_de_poste,\n",
    "        inputs=[fiche_input, objectif_input],\n",
    "        outputs=output\n",
    "    )\n",
    "\n",
    "    # Upload fiche de poste=\n",
    "    comment='''\n",
    "    fiche_input = gr.File(label=\"📄 Charger une fiche de poste (PDF ou .txt)\")\n",
    "    btn_fiche = gr.Button(\"📥 Lire la fiche de poste\")\n",
    "    fiche_resume = gr.Textbox(label=\"📝 Résumé de la fiche de poste\", lines=6, interactive=False)\n",
    "    btn_fiche.click(fn=lire_fiche_de_poste, inputs=[fiche_input], outputs=[fiche_resume])'''\n",
    "   \n",
    "\n",
    "    question_display = gr.Textbox(label=\"🧑‍💼 Question posée par le recruteur\", interactive=False)\n",
    "    reponse_input = gr.Textbox(label=\"🎤 Votre réponse\", placeholder=\"Tapez votre réponse ici...\")\n",
    "\n",
    "    bouton_suivant = gr.Button(\"➡️ Question suivante\")\n",
    "    bouton_export = gr.Button(\"💾 Exporter les réponses\")\n",
    "    export_result = gr.Textbox(label=\"Fichier exporté\", interactive=False)\n",
    "\n",
    "    bouton_suivant.click(fn=entretien_interactif, \n",
    "                         inputs=[poste_input, model_input, reponse_input], \n",
    "                         outputs=[question_display])\n",
    "\n",
    "    bouton_export.click(fn=exporter_log, inputs=[], outputs=[export_result])\n",
    "\n",
    "    bouton_analyse = gr.Button(\"🧠 Analyse des réponses\")\n",
    "    feedback_output = gr.Textbox(label=\"🧠 Feedback RH personnalisé\", lines=8, interactive=False)\n",
    "\n",
    "    bouton_softskills = gr.Button(\"📊 Rapport Soft Skills\")\n",
    "    softskills_output = gr.Textbox(label=\"📊 Évaluation des soft skills\", lines=8, interactive=False)\n",
    "\n",
    "    bouton_analyse.click(fn=analyser_entretien, inputs=[], outputs=[feedback_output])\n",
    "    bouton_softskills.click(fn=eval_soft_skills, inputs=[], outputs=[softskills_output])\n",
    "\n",
    "    bouton_note = gr.Button(\"🎯 Générer note globale\")\n",
    "    note_output = gr.Textbox(label=\"🎯 Résultat de la performance\", lines=3)\n",
    "\n",
    "    bouton_pdf = gr.Button(\"📄 Télécharger le rapport PDF\")\n",
    "    pdf_output = gr.File(label=\"📥 Rapport PDF téléchargeable\")\n",
    "\n",
    "    bouton_note.click(fn=generer_note_globale, inputs=[], outputs=[note_output])\n",
    "    # bouton_pdf.click(fn=generer_pdf_entretien, inputs=[], outputs=[pdf_output]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499bc2f7-5bd8-453d-93ed-0402236c4127",
   "metadata": {},
   "source": [
    "### Lancement de l'interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0395ed2-9c9b-4bc2-b0cb-ff746711d382",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.launch(share=True)  # Ajoute share=True pour un lien public sur smartphone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08eeb29-5f5a-4bdd-8f0d-615d9a05250f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
