{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "933703df-7418-444f-9206-131ff1372615",
   "metadata": {},
   "source": [
    "# TP1 : Introduction aux LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be280aa4-b908-4406-8565-e4f5041cb36a",
   "metadata": {},
   "source": [
    "## 1. Installation des d√©pendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18881969-3d88-43cb-aa0b-1acbb486fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio openai \n",
    "!pip install transformers datasets torch datasets\n",
    "!pip install scikit-learn pandas\n",
    "!pip install fpdf fitz frontend pillow pdfplumber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c400b3-5164-4522-b069-309673f99563",
   "metadata": {},
   "source": [
    "## Instanciation d'un mod√®le LLM depuis Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1007b894-fde2-46b7-a9cc-debaae31bedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©e un client qui se connecte √† l'API Groq via  API key\n",
    "import os\n",
    "import openai\n",
    "\n",
    "# pr√©requis: cr√©er un compte  et g√©n√©rer un token https://console.groq.com/keys\n",
    "\n",
    "if not os.environ.get(\"GROQ_API_KEY\"):\n",
    "  os.environ[\"GROQ_API_KEY\"] = getpass.getpass('Enter your groq api key')\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    base_url=\"https://api.groq.com/openai/v1\",\n",
    "    api_key=os.environ[\"GROQ_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b179b7f-076c-4450-9d1f-506206e052bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_choosed=\"meta-llama/llama-4-scout-17b-16e-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd368ee-4749-4ffd-b8f2-8cd8d0433900",
   "metadata": {},
   "source": [
    "## A. Simulation d'Entretien "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3427909d-3489-4eab-aa60-936dfb72b78a",
   "metadata": {},
   "source": [
    "‚Ä¢\tG√©n√©ration de Questions : Utilisation d‚Äôun LLM l√©ger pour g√©n√©rer dynamiquement des questions d‚Äôentretien en fonction du poste ou du profil souhait√©. \n",
    "‚Ä¢\tInteraction Conversationnelle : L‚Äôassistant peut jouer le r√¥le d‚Äôun recruteur en posant des questions et en enregistrant les r√©ponses du candidat.\n",
    "\n",
    "==> G√©n√©ration de questions dynamiques √† l‚Äôaide d‚Äôun LLM l√©ger local (ou via transformers en fallback)\n",
    "\n",
    "==> Interaction sous forme de Q/R avec transcription manuelle simul√©e (ou audio √† ajouter ensuite)\n",
    "\n",
    "==> Stockage des questions et r√©ponses dans un log structur√© (en vue d‚Äôune future analyse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fb4f4c-9248-4b2c-98de-b3132cb86938",
   "metadata": {},
   "source": [
    "Pr√©requis :\n",
    "    -transformers (pip install transformers)\n",
    "    -(Optionnel) un LLM local comme Mistral ou TinyLLaMA via Ollama ou llama-cpp-python\n",
    "    -whisper pour l‚Äôaudio + transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a39b37d-2ec7-4373-96ff-218158863145",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e13315-accd-47c8-9d97-8c3705ab92bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44921c18-3616-4285-b507-66036248f91d",
   "metadata": {},
   "source": [
    "### Chat with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67e8440-c847-4518-84bf-1fe68c65854b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction g√©n√©rique pour interagir avec le mod√®le en streaming\n",
    "def chat_with_model(prompt, display=True):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    stream = client.chat.completions.create(\n",
    "        model=model_choosed,  \n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        content = chunk.choices[0].delta.content\n",
    "        if content:\n",
    "            response += content\n",
    "            if display:\n",
    "                print(content, end=\"\")\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43071ba3-4d85-4c50-af01-22997a672d3a",
   "metadata": {},
   "source": [
    "### Fonction test si API / requ√™te modele fonctionne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01921ffe-aea1-44b2-8326-acf7f9dbc2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt= \"Peux-tu me raconter une blague ? \"\n",
    "reponse = chat_with_model(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85a3066-ab9d-418b-bfe4-b6f7a7037c00",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Simulation interactive de l‚Äôentretien"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da194f8f-cf2a-4bcc-ae5e-b0b14e5297a7",
   "metadata": {},
   "source": [
    "### G√©n√©rateur de questions par rapport √† un poste avec Interface gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8899268-f9c4-49b9-9347-6cdb27b098d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Variables de session\n",
    "session_log = {\n",
    "    \"profil\": \"\",\n",
    "    \"questions_reponses\": [],\n",
    "    \"previous_qs\": [],\n",
    "}\n",
    "\n",
    "# G√©n√©ration de question (m√™me logique que plus t√¥t)\n",
    "def generate_question_gradio(profil, model):\n",
    "    global model_choosed\n",
    "    model_choosed = model\n",
    "    prompt = f\"\"\"Tu es un recruteur exp√©riment√©. Pose une nouvelle question d'entretien √† un candidat pour un poste de {profil}.\n",
    "√âvite de r√©p√©ter les questions pr√©c√©dentes : {', '.join(session_log['previous_qs']) if session_log['previous_qs'] else 'aucune'}.\n",
    "Donne uniquement la question, sans introduction, sans commentaire.\"\"\"\n",
    "    \n",
    "    question = chat_with_model(prompt, display=False).strip()\n",
    "    session_log[\"previous_qs\"].append(question)\n",
    "    return question\n",
    "\n",
    "# Fonction principale d‚Äôinteraction (question ‚Üî r√©ponse)\n",
    "def entretien_interactif(profil, model, reponse_utilisateur):\n",
    "    if not session_log[\"profil\"]:\n",
    "        session_log[\"profil\"] = profil\n",
    "\n",
    "    if reponse_utilisateur and session_log[\"questions_reponses\"]:\n",
    "        # Sauvegarde r√©ponse pr√©c√©dente\n",
    "        session_log[\"questions_reponses\"][-1][\"reponse\"] = reponse_utilisateur\n",
    "\n",
    "    # G√©n√®re une nouvelle question\n",
    "    question = generate_question_gradio(profil, model)\n",
    "    session_log[\"questions_reponses\"].append({\n",
    "        \"question\": question,\n",
    "        \"reponse\": \"\"\n",
    "    })\n",
    "    return question\n",
    "\n",
    "# Fonction pour exporter les logs\n",
    "def exporter_log():\n",
    "    file_name = f\"entretien_{session_log['profil'].replace(' ', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(session_log, f, indent=2, ensure_ascii=False)\n",
    "    return file_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69812ed1-db6f-420a-bd21-95bde1fbd6c0",
   "metadata": {},
   "source": [
    "### Fonction analyse automatique des r√©ponses (LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a0e865-7fd7-4b62-8f93-4ef4acdf3057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyser_entretien():\n",
    "    if not session_log[\"questions_reponses\"]:\n",
    "        return \"Aucune r√©ponse √† analyser.\"\n",
    "\n",
    "    # G√©n√®re le contenu brut de l'entretien pour analyse\n",
    "    entretien_text = \"\\n\".join([\n",
    "        f\"Q: {pair['question']}\\nR: {pair['reponse']}\" \n",
    "        for pair in session_log[\"questions_reponses\"]\n",
    "    ])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Tu es un expert RH. Analyse les r√©ponses suivantes donn√©es lors d‚Äôun entretien pour un poste de {session_log['profil']} :\n",
    "\n",
    "{entretien_text}\n",
    "\n",
    "Donne un retour constructif :\n",
    "- Points forts\n",
    "- Points √† am√©liorer\n",
    "- Conseils pour le prochain entretien\n",
    "R√©ponds de fa√ßon bienveillante et professionnelle.\n",
    "\"\"\"\n",
    "    feedback = chat_with_model(prompt, display=False)\n",
    "    return feedback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee5547a-f76c-4e83-9197-a23310b91438",
   "metadata": {},
   "source": [
    "### Fonction √âvaluation des soft skills d√©tect√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70524540-15a1-4581-88be-373224984ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_soft_skills():\n",
    "    entretien_text = \"\\n\".join([\n",
    "        f\"Q: {pair['question']}\\nR: {pair['reponse']}\" \n",
    "        for pair in session_log[\"questions_reponses\"]\n",
    "    ])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Lis cet √©change d‚Äôentretien pour un poste de {session_log['profil']} :\n",
    "\n",
    "{entretien_text}\n",
    "\n",
    "Identifie les soft skills per√ßues dans les r√©ponses du candidat (exemples : communication, leadership, adaptabilit√©, r√©solution de probl√®mes...).\n",
    "Pr√©sente les r√©sultats sous forme de liste avec un niveau estim√© pour chaque comp√©tence (faible, moyen, fort) et un commentaire.\n",
    "\"\"\"\n",
    "    soft_skills_feedback = chat_with_model(prompt, display=False)\n",
    "    return soft_skills_feedback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca475bc-d1a2-4c43-9a34-fc4e701a440a",
   "metadata": {},
   "source": [
    "### G√©n√©ration de note globale de performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec562b3-bde3-441d-9a64-8d23287765ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generer_note_globale():\n",
    "    entretien_text = \"\\n\".join([\n",
    "        f\"Q: {pair['question']}\\nR: {pair['reponse']}\" \n",
    "        for pair in session_log[\"questions_reponses\"]\n",
    "    ])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Voici un entretien pour un poste de {session_log['profil']} :\n",
    "\n",
    "{entretien_text}\n",
    "\n",
    "Attribue une note globale de performance (sur 10) au candidat, suivie d'un r√©sum√© de l'√©valuation. Sois professionnel et bienveillant.\n",
    "Exemple de sortie : \"Note : 7/10 - Bon niveau technique, mais manque de pr√©cision sur certains points...\"\n",
    "\"\"\"\n",
    "    result = chat_with_model(prompt, display=False)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d753a8-0a21-4adf-9bda-18d3f996d877",
   "metadata": {},
   "source": [
    "### G√©n√©ration + T√©l√©chargement d‚Äôun PDF r√©capitulatif "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cde2f8-d3f7-4e22-8ef7-9a4182c43dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpdf import FPDF\n",
    "import tempfile\n",
    "\n",
    "def generer_pdf_entretien():\n",
    "    pdf = FPDF()\n",
    "    pdf.add_page()\n",
    "    pdf.set_auto_page_break(auto=True, margin=15)\n",
    "\n",
    "    pdf.set_font(\"Arial\", 'B', 16)\n",
    "    pdf.cell(0, 10, f\"Compte-rendu d'entretien - {session_log['profil']}\", ln=True)\n",
    "\n",
    "    pdf.set_font(\"Arial\", size=12)\n",
    "    pdf.ln(5)\n",
    "\n",
    "    for i, pair in enumerate(session_log[\"questions_reponses\"], 1):\n",
    "        pdf.multi_cell(0, 10, f\"Q{i} : {pair['question']}\")\n",
    "        pdf.multi_cell(0, 10, f\"R{i} : {pair['reponse']}\")\n",
    "        pdf.ln(2)\n",
    "\n",
    "    # Feedback RH\n",
    "    feedback = analyser_entretien()\n",
    "    pdf.set_font(\"Arial\", 'B', 14)\n",
    "    pdf.cell(0, 10, \"üß† Feedback RH\", ln=True)\n",
    "    pdf.set_font(\"Arial\", size=12)\n",
    "    pdf.multi_cell(0, 10, feedback)\n",
    "\n",
    "    # Soft Skills\n",
    "    softskills = eval_soft_skills()\n",
    "    pdf.set_font(\"Arial\", 'B', 14)\n",
    "    pdf.cell(0, 10, \"üìä Soft Skills d√©tect√©es\", ln=True)\n",
    "    pdf.set_font(\"Arial\", size=12)\n",
    "    pdf.multi_cell(0, 10, softskills)\n",
    "\n",
    "    # Note globale\n",
    "    note = generer_note_globale()\n",
    "    pdf.set_font(\"Arial\", 'B', 14)\n",
    "    pdf.cell(0, 10, \"üéØ Note globale de performance\", ln=True)\n",
    "    pdf.set_font(\"Arial\", size=12)\n",
    "    pdf.multi_cell(0, 10, note)\n",
    "\n",
    "    # Sauvegarde temporaire\n",
    "    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\")\n",
    "    pdf.output(temp_file.name)\n",
    "    return temp_file.name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb17fcf-caf4-48be-a22d-9ab21c9b03fe",
   "metadata": {},
   "source": [
    "### Fonctionnalit√© : Upload de photo du candidat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2649ac17-09f9-4c12-9416-7f88f46dae26",
   "metadata": {},
   "source": [
    "Exemples de consignes personnalisables :\n",
    "‚ÄúCette personne inspire-t-elle confiance pour un entretien ?‚Äù\n",
    "\n",
    "‚ÄúCette tenue est-elle adapt√©e √† un poste en entreprise ?‚Äù\n",
    "\n",
    "‚ÄúFais une analyse non-verbale comme un coach en image.‚Äù"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fae82c84-104f-44e4-b5f5-a3778730b2d7",
   "metadata": {},
   "source": [
    "1 token ‚âà 4 caract√®res ‚Üí 100 Ko JPEG ‚Üí ~100k tokens üò¨\n",
    "Or Groq (comme OpenAI) limite √† 32 768 tokens max pour inputs + outputs\n",
    "==> forcer une image < 30 Ko encod√©e en base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0729006a-0c3e-4980-aead-fa0f935eeb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def analyser_photo_candidat(image_path, commentaire=\"Analyse cette photo pour un entretien : posture, tenue, expression.\"):\n",
    "    if not image_path:\n",
    "        return \"Aucune image charg√©e.\"\n",
    "\n",
    "    # Encode l'image en base64\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        image_bytes = f.read()\n",
    "        image_b64 = base64.b64encode(image_bytes).decode('utf-8')\n",
    "    \n",
    "    image_data_url = f\"data:image/jpeg;base64,{image_b64}\"  # ou image/png selon le type\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": commentaire\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": image_data_url\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Appel au mod√®le Vision (ex: llava-1.5-7b ou autre)\n",
    "    stream = client.chat.completions.create(\n",
    "        model=model_choosed,  # mod√®le vision compatible Groq\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    result = \"\"\n",
    "    for chunk in stream:\n",
    "        content = chunk.choices[0].delta.content\n",
    "        if content:\n",
    "            result += content\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1399cd05-6a9e-43ec-8167-277ca2bcea91",
   "metadata": {},
   "source": [
    "### [KO] Fonctionnalit√© : Upload de photo du candidat avec compression si > 30ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9738609-709f-4562-bd5f-a640cad01841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: KO compression\n",
    "def analyser_photo_candidat_avec_preview(image_path, commentaire=\"Analyse cette photo pour un entretien : posture, tenue, expression.\"):\n",
    "    if not image_path:\n",
    "        return \"Aucune image charg√©e.\", None\n",
    "\n",
    "    # √âtape 1 : Compression + redimensionnement\n",
    "    with Image.open(image_path) as img:\n",
    "        img = img.convert(\"RGB\")\n",
    "        img = img.resize((224, 224))\n",
    "        buffered = io.BytesIO()\n",
    "        img.save(buffered, format=\"JPEG\", quality=30, optimize=True)\n",
    "        image_bytes = buffered.getvalue()\n",
    "\n",
    "        # Pour Gradio, on retourne un objet PIL directement ici\n",
    "        img_preview = Image.open(io.BytesIO(image_bytes))\n",
    "\n",
    "    # V√©rif taille\n",
    "    compressed_size_kb = len(image_bytes) / 1024\n",
    "    if compressed_size_kb > 40:\n",
    "        return f\"‚ùå Image compress√©e trop lourde ({compressed_size_kb:.1f} Ko).\", img_preview\n",
    "\n",
    "    # Encodage base64 pour le LLM\n",
    "    image_b64 = base64.b64encode(image_bytes).decode('utf-8')\n",
    "    image_data_url = f\"data:image/jpeg;base64,{image_b64}\"\n",
    "\n",
    "    # Construction du message pour le LLM\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": commentaire},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": image_data_url}}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Appel API\n",
    "    stream = client.chat.completions.create(\n",
    "        model=model_choosed,\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    result = \"\"\n",
    "    for chunk in stream:\n",
    "        content = chunk.choices[0].delta.content\n",
    "        if content:\n",
    "            result += content\n",
    "\n",
    "    return result, img_preview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b753085f-635e-4255-9383-f727c9400eb7",
   "metadata": {},
   "source": [
    "### Charger une fiche de poste (texte)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "018026f4-d108-4211-ac70-12e45be22954",
   "metadata": {},
   "source": [
    "G√©n√©rer des questions sp√©cifiques au poste\n",
    "Adapter l‚Äôanalyse des r√©ponses √† ce profil\n",
    "Enrichir la synth√®se RH et le rapport PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5c4217-1ecf-47eb-879d-7bed4d228eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traiter_fiche_de_poste(texte_fiche, objectif=\"G√©n√®re 5 questions d'entretien adapt√©es √† ce poste\"):\n",
    "    if not texte_fiche.strip():\n",
    "        return \"üõë Merci de coller le contenu de la fiche de poste.\"\n",
    "\n",
    "    # üîí Limiter la taille du texte pour √©viter les erreurs du mod√®le\n",
    "    max_chars = 4000  # Ajustable selon le mod√®le\n",
    "    if len(texte_fiche) > max_chars:\n",
    "        return f\"‚ö†Ô∏è La fiche est trop longue ({len(texte_fiche)} caract√®res). Limite : {max_chars}.\"\n",
    "\n",
    "    try:\n",
    "        prompt = f\"{objectif}\\n\\nVoici la fiche de poste :\\n{texte_fiche}\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "\n",
    "        stream = client.chat.completions.create(\n",
    "            model=model_choosed,\n",
    "            messages=messages,\n",
    "            stream=True\n",
    "        )\n",
    "\n",
    "        result = \"\"\n",
    "        for chunk in stream:\n",
    "            content = chunk.choices[0].delta.content\n",
    "            if content:\n",
    "                result += content\n",
    "\n",
    "        return result if result.strip() else \"‚ö†Ô∏è R√©ponse vide du mod√®le.\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Erreur : {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80afbd3f-15f0-4ba3-b2bb-6c40cc332846",
   "metadata": {},
   "source": [
    "### [KO] Charger une fiche de poste (pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7f03a7-29d1-4780-83b1-bf0eed4fb288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF pour lire les PDF\n",
    "\n",
    "def lire_fiche_de_poste(fichier):\n",
    "    if not fichier:\n",
    "        return \"Aucun fichier fourni.\"\n",
    "\n",
    "    content = \"\"\n",
    "    if fichier.name.endswith(\".pdf\"):\n",
    "        doc = fitz.open(fichier.name)\n",
    "        content = \"\\n\".join([page.get_text() for page in doc])\n",
    "    elif fichier.name.endswith(\".txt\"):\n",
    "        with open(fichier.name, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "    else:\n",
    "        return \"Format non support√©. Utilisez un PDF ou un .txt.\"\n",
    "\n",
    "    # On r√©sume le contenu avec le LLM\n",
    "    prompt = f\"\"\"\n",
    "Voici une fiche de poste. R√©sume-la en identifiant :\n",
    "- Le poste\n",
    "- Les comp√©tences cl√©s attendues\n",
    "- Les missions principales\n",
    "- Le profil id√©al\n",
    "\n",
    "Fiche :\n",
    "{content}\n",
    "\"\"\"\n",
    "    resume = chat_with_model(prompt, display=False)\n",
    "\n",
    "    # On stocke dans la session pour l‚Äôutiliser dans la g√©n√©ration des questions\n",
    "    session_log[\"profil\"] = resume\n",
    "\n",
    "    return resume\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a4cc28-b909-46b7-a441-d2d3bcadc17b",
   "metadata": {},
   "source": [
    "### [TODO] int√©gration fiche de poste dans la g√©n√©ration des questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca91fad-444a-49d0-ab8f-21457aae3645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# int√©gration dans la g√©n√©ration des questions\n",
    "prompt = f\"\"\"\n",
    "Tu joues le r√¥le d'un recruteur. G√©n√®re une question d'entretien adapt√©e √† ce profil :\n",
    "\n",
    "{session_log['profil']}\n",
    "\n",
    "Pose une question √† la fois. N‚Äôindique que la question sans commentaire ni r√©ponse.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e686048-37ee-4e25-9f1a-331803ad10b6",
   "metadata": {},
   "source": [
    "### Interface utilisateur GRADIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581abb93-34a0-4cbb-b2ae-2bba05441227",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks(title=\"Assistant d'entretien IA\") as demo:\n",
    "    gr.Markdown(\"## ü§ñ Assistant IA d'entra√Ænement √† l'entretien d'embauche\")\n",
    "\n",
    "    with gr.Row():\n",
    "        poste_input = gr.Textbox(label=\"Profil / Poste vis√©\", placeholder=\"ex: ing√©nieur IA, d√©veloppeur web...\")\n",
    "        model_input = gr.Dropdown([model_choosed], \n",
    "                                  label=\"Mod√®le utilis√©\", value=model_choosed)\n",
    "\n",
    "    # analyse photo\n",
    "    gr.Markdown(\"## üì∏ Analyse Visuelle du Candidat\")\n",
    "    photo_input = gr.Image(label=\"üì∑ Charge une photo\", type=\"filepath\")\n",
    "    feedback_photo = gr.Textbox(label=\"üß† Analyse de la photo\", lines=5)\n",
    "    bouton_photo = gr.Button(\"Analyser la photo\")\n",
    "\n",
    "    bouton_photo.click(fn=analyser_photo_candidat, inputs=[photo_input], outputs=[feedback_photo])\n",
    "\n",
    "    # Upload fiche de post sous txt\n",
    "    gr.Markdown(\"## üìã Analyse de fiche de poste (coller le texte)\")\n",
    "    \n",
    "    fiche_input = gr.Textbox(lines=10, placeholder=\"Collez ici le contenu de la fiche de poste\", label=\"üìù Fiche de poste\")\n",
    "    objectif_input = gr.Textbox(value=\"G√©n√®re 5 questions d'entretien adapt√©es √† ce poste\", label=\"üéØ Objectif de g√©n√©ration\")\n",
    "    \n",
    "    bouton = gr.Button(\"üîç Analyser\")\n",
    "    output = gr.Textbox(label=\"üìä R√©sultat\")\n",
    "\n",
    "    bouton.click(\n",
    "        fn=traiter_fiche_de_poste,\n",
    "        inputs=[fiche_input, objectif_input],\n",
    "        outputs=output\n",
    "    )\n",
    "\n",
    "    # Upload fiche de poste=\n",
    "    comment='''\n",
    "    fiche_input = gr.File(label=\"üìÑ Charger une fiche de poste (PDF ou .txt)\")\n",
    "    btn_fiche = gr.Button(\"üì• Lire la fiche de poste\")\n",
    "    fiche_resume = gr.Textbox(label=\"üìù R√©sum√© de la fiche de poste\", lines=6, interactive=False)\n",
    "    btn_fiche.click(fn=lire_fiche_de_poste, inputs=[fiche_input], outputs=[fiche_resume])'''\n",
    "   \n",
    "\n",
    "    question_display = gr.Textbox(label=\"üßë‚Äçüíº Question pos√©e par le recruteur\", interactive=False)\n",
    "    reponse_input = gr.Textbox(label=\"üé§ Votre r√©ponse\", placeholder=\"Tapez votre r√©ponse ici...\")\n",
    "\n",
    "    bouton_suivant = gr.Button(\"‚û°Ô∏è Question suivante\")\n",
    "    bouton_export = gr.Button(\"üíæ Exporter les r√©ponses\")\n",
    "    export_result = gr.Textbox(label=\"Fichier export√©\", interactive=False)\n",
    "\n",
    "    bouton_suivant.click(fn=entretien_interactif, \n",
    "                         inputs=[poste_input, model_input, reponse_input], \n",
    "                         outputs=[question_display])\n",
    "\n",
    "    bouton_export.click(fn=exporter_log, inputs=[], outputs=[export_result])\n",
    "\n",
    "    bouton_analyse = gr.Button(\"üß† Analyse des r√©ponses\")\n",
    "    feedback_output = gr.Textbox(label=\"üß† Feedback RH personnalis√©\", lines=8, interactive=False)\n",
    "\n",
    "    bouton_softskills = gr.Button(\"üìä Rapport Soft Skills\")\n",
    "    softskills_output = gr.Textbox(label=\"üìä √âvaluation des soft skills\", lines=8, interactive=False)\n",
    "\n",
    "    bouton_analyse.click(fn=analyser_entretien, inputs=[], outputs=[feedback_output])\n",
    "    bouton_softskills.click(fn=eval_soft_skills, inputs=[], outputs=[softskills_output])\n",
    "\n",
    "    bouton_note = gr.Button(\"üéØ G√©n√©rer note globale\")\n",
    "    note_output = gr.Textbox(label=\"üéØ R√©sultat de la performance\", lines=3)\n",
    "\n",
    "    bouton_pdf = gr.Button(\"üìÑ T√©l√©charger le rapport PDF\")\n",
    "    pdf_output = gr.File(label=\"üì• Rapport PDF t√©l√©chargeable\")\n",
    "\n",
    "    bouton_note.click(fn=generer_note_globale, inputs=[], outputs=[note_output])\n",
    "    # bouton_pdf.click(fn=generer_pdf_entretien, inputs=[], outputs=[pdf_output]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499bc2f7-5bd8-453d-93ed-0402236c4127",
   "metadata": {},
   "source": [
    "### Lancement de l'interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0395ed2-9c9b-4bc2-b0cb-ff746711d382",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.launch(share=True)  # Ajoute share=True pour un lien public sur smartphone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08eeb29-5f5a-4bdd-8f0d-615d9a05250f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
